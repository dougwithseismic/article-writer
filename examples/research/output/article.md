# How Large Language Models Are Reshaping Software Engineering Productivity

*A data-driven analysis of LLM-assisted development across code generation, debugging, documentation, and team workflows — with findings from 40+ studies and real-world engineering organizations*

---

## The Productivity Promise Nobody Has Verified

GitHub Copilot now generates [46% of code](https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/) in files where it is enabled. Fifteen million developers use it daily. And according to GitHub's own research, developers using Copilot complete tasks [55% faster](https://arxiv.org/abs/2302.06590) than those working without it — a statistically significant finding (P=.0017) drawn from a controlled experiment with 95 professional developers. Google's CEO told investors that AI now writes [over 30% of the company's new code](https://www.entrepreneur.com/business-news/ai-is-taking-over-coding-at-microsoft-google-and-meta/490896), up from 25% just six months prior. Microsoft reports similar numbers. The narrative seems settled: LLM coding assistants are a productivity revolution, and organizations that fail to adopt them will fall behind. Amazon has pushed CodeWhisperer across its development teams. Replit's agent-first platform created [5 million applications in 2025](https://blog.replit.com/2025-replit-in-review), with 500,000 businesses actively using the platform. The [2025 Stack Overflow Developer Survey](https://survey.stackoverflow.co/2025/ai) found that 84% of developers are now using or planning to use AI tools in their workflows, up from 76% the year before. By any adoption metric, AI-assisted development has crossed the threshold from experiment to standard practice.

Except the controlled evidence tells a different story. In July 2025, METR — an independent AI research organization — published the results of a [randomized controlled trial](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) involving 16 experienced open-source developers who completed 246 real tasks on repositories where they averaged five years of prior experience. Developers were randomly assigned to use or not use AI tools (primarily Cursor Pro with Claude 3.5/3.7 Sonnet) for each task. The result: developers using AI were [19% slower](https://arxiv.org/abs/2507.09089) than those working without it. More striking still, after completing their tasks, developers estimated that AI had sped them up by approximately 20%. The perception gap — 39 percentage points between what developers believed and what actually happened — suggests that even experienced engineers cannot reliably assess whether these tools are helping them.

This disconnect between vendor claims and independent findings is not an isolated anomaly. A [systematic literature review](https://arxiv.org/abs/2507.03156) published in 2025 examined 37 peer-reviewed studies on LLM-assisted developer productivity and found deeply mixed results. While certain task categories showed consistent gains — minimized code search, automation of repetitive patterns — the review also identified significant concerns around cognitive offloading, reduced team collaboration, and inconsistent effects on code quality. The [2025 Stack Overflow Developer Survey](https://survey.stackoverflow.co/2025/ai) captures this tension quantitatively: 84% of developers now use or plan to use AI coding tools, up from 76% in 2024. Yet trust in AI accuracy has [fallen to just 29%](https://stackoverflow.co/company/press/archive/stack-overflow-2025-developer-survey/), down from 40% the prior year. Forty-six percent of developers actively distrust AI output. Positive sentiment toward AI tools dropped from above 70% in 2023 to roughly 60% in 2025. The single largest frustration, cited by 66% of respondents, is dealing with AI solutions that are almost right but not quite — which then creates a second problem, as 45% report that debugging AI-generated code consumes more time than writing code themselves.

You might argue that the METR study was small — 16 developers is not a definitive sample. That is a fair critique. But the study's methodology was unusually rigorous: developers worked on their own repositories, on tasks they had already identified as valuable, with screen recordings validating reported times. And the perception gap finding has been independently corroborated. A [large-scale controlled experiment](https://arxiv.org/html/2410.12944v2) across Microsoft, Accenture, and another enterprise company involving 4,867 developers found that Copilot users completed 26.08% more tasks weekly — a positive result, but one achieved on tasks that were assigned and scoped in advance, not self-directed work on complex systems. When you compare findings across study designs, a pattern emerges: the more a study resembles real engineering work — with ambiguity, existing code, and architectural complexity — the smaller the measured gains, and in some cases, the gains disappear entirely.

The gap between what vendors report and what independent researchers measure does not mean LLM coding assistants are useless. It means the industry has been asking the wrong question. Rather than measuring whether AI tools make developers faster in isolation, the productive question is where, for whom, and under what conditions these tools deliver genuine engineering value across the full development lifecycle. That is what this article sets out to answer, drawing on more than 40 studies, industry reports, and real-world deployment data from organizations including Google, Microsoft, Booking.com, Palo Alto Networks, and ZoomInfo.

---

## How We Got the Metrics Wrong

The problem begins with how the software industry measures developer productivity in the first place. In 2021, Nicole Forsgren, Margaret-Anne Storey, and colleagues at GitHub and Microsoft Research published the [SPACE framework](https://queue.acm.org/detail.cfm?id=3454124) in ACM Queue, arguing that developer productivity spans five interrelated dimensions: Satisfaction and well-being, Performance, Activity, Communication and collaboration, and Efficiency and flow. Their central argument was that productivity cannot be captured by any single metric — a lesson the field has largely ignored when evaluating LLM tools. The parallel [DORA metrics](https://dora.dev/guides/dora-metrics-four-keys/) framework, which tracks deployment frequency, lead time for changes, change failure rate, and mean time to recovery, offers a delivery-focused lens that vendor studies have similarly sidestepped.

Consider how most vendor-sponsored productivity studies work. GitHub's widely cited experiment measured how long 95 developers took to complete a self-contained HTTP server task. Google's internal randomized controlled trial, conducted in [June and July 2024](https://arxiv.org/html/2410.12944v2) with full-time engineers who had at least one year of tenure, measured time-to-completion on individual development tasks. These designs are methodologically clean, but they systematically sacrifice realism for experimental control. The tasks are self-contained. They do not require prior context or deep familiarity with an existing codebase. They use evaluation metrics that do not capture many of the capabilities that define real engineering work — architectural reasoning, cross-system integration, handling of edge cases that only emerge from domain knowledge. As the METR researchers [noted](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/), the factors that likely drove their counterintuitive slowdown result — over-optimism about AI capabilities, LLMs interfering with existing codebase knowledge, poor performance on large repositories, and inability to leverage tacit context — are precisely the factors that controlled lab experiments are designed to exclude.

The [2024 DORA report](https://cloud.google.com/blog/products/devops-sre/announcing-the-2024-dora-report) makes this disconnect quantifiable at the organizational level. Teams using AI coding tools completed 21% more tasks and merged [98% more pull requests](https://www.faros.ai/blog/key-takeaways-from-the-dora-report-2025). Individual output went up by every activity metric that matters to vendor sales pitches. But delivery throughput — the organizational metric that actually correlates with business outcomes — decreased by an estimated 1.5%. Delivery stability fell by 7.2%. The report called this the "AI Productivity Paradox": more code produced, but the same or worse organizational outcomes. The data suggest that improving individual development speed does not automatically improve software delivery, at least not without adherence to fundamentals like small batch sizes and robust testing.

What appears to happen instead is task shifting. A [longitudinal study](https://arxiv.org/html/2511.04427v1) tracking 806 repositories that adopted the Cursor AI client between August 2024 and March 2025 found that adoption produced substantial but transient velocity gains alongside persistent increases in technical debt. The initial productivity surge gave way to a self-reinforcing cycle in which accumulated debt dampened future velocity. Producing code is only one part of software engineering, and when you dramatically accelerate that one part, the bottlenecks migrate elsewhere — to code review, to debugging, to architectural decisions that AI-generated code often sidesteps rather than solves.

This is not a novel insight in engineering management, but it takes on new urgency when the acceleration is as dramatic as LLM tools provide. Google's own internal data illustrates the dynamic. Their randomized controlled trial found a [best estimate of 21% faster task completion](https://arxiv.org/html/2410.12944v2), but with a large confidence interval, and the gains varied enormously by developer tenure. Newer developers with less than a year of experience saw speedups of 35% to 39%, while veterans saw improvements in the 8% to 16% range. The explanation is intuitive: experienced developers already have the mental models and shortcuts that AI provides to novices. When an AI tool offers suggestions that contradict a veteran's understanding of a codebase's conventions, the time spent evaluating and rejecting those suggestions can exceed the time saved by accepting them.

Booking.com offers one of the more rigorous examples of organizational measurement done well. The company deployed AI coding tools to over [3,500 engineers](https://getdx.com/customers/booking-uses-dx-to-measure-impact-of-genai/) and partnered with DX, a developer intelligence platform, to track impact across multiple dimensions. They found that developers who used AI daily achieved 16% higher code throughput than those who did not, and that developer satisfaction with AI tooling rose 15 points over six months. Critically, they measured what developers did with saved time: AI users redirected effort from routine tasks toward higher-value work. This multi-dimensional approach — measuring throughput, satisfaction, and task reallocation simultaneously — represents the kind of methodology that single-metric vendor studies cannot provide.

The [Atlassian State of Developer Experience Report 2025](https://www.atlassian.com/blog/developer/developer-experience-report-2025), based on surveys of 3,500 developers across six countries, reveals the paradox in its sharpest form. Ninety-nine percent of developers reported saving time with AI, and 68% said they saved ten or more hours per week — a dramatic increase from 38% the previous year. At the same time, 50% of developers reported losing ten or more hours per week to non-coding tasks: finding information about services and APIs, adapting to new technology, and context switching between tools. Developers are saving ten hours a week with AI and losing ten hours a week to organizational friction. Sixty-three percent of developers said that their engineering leaders do not understand their actual pain points — up from 44% the previous year. This growing disconnect suggests that leaders are banking the perceived time savings from AI tooling without addressing the systemic friction that eats those savings back. Until engineering leaders measure both sides of that equation — the speed gained and the friction endured — productivity claims will continue to mislead.

The measurement problem is not merely academic. It shapes billions of dollars in tool licensing decisions, hiring strategy, and organizational design. When a CTO reads that AI tools produce a 55% speedup and approves enterprise-wide licensing, that decision is built on a measurement foundation that may be systematically biased. When that same CTO sees code review throughput declining six months later but cannot connect the trend to AI-generated code volume, the measurement failure has become an operational problem. What follows is an examination of the specific areas where the evidence is most solid — and where it is most misleading.

---

## What Code Generation Actually Speeds Up

The evidence on LLM-assisted code generation is neither as positive as vendors claim nor as negative as the METR study might suggest. The reality is granular: acceptance rates, defect rates, and productivity gains vary dramatically depending on programming language, task complexity, and developer experience level.

Start with acceptance rates — the percentage of AI-generated suggestions that developers actually keep. Data from GitHub and academic studies show significant variation across languages. Java developers accept Copilot suggestions at the highest rate, between [57.7% and 61%](https://www.secondtalent.com/resources/github-copilot-statistics/) depending on the study. JavaScript follows at 54.1%, Python at 41.0%, and C at just 29.7%. These numbers reflect something meaningful about where LLMs excel: languages with more boilerplate, stronger conventions, and extensive public training data produce better suggestions. The overall acceptance rate across all contexts ranges from 27% to 30%, and it shifts upward during non-working hours when developers tend to tackle less complex tasks. That last detail is telling — it suggests that acceptance rates are as much a function of task characteristics as they are of model capability. When developers work on simpler tasks, they accept more suggestions. The tool does not become better at night; the work becomes more amenable to automation.

Task difficulty reveals an even sharper boundary. Research evaluating Copilot's correctness across problem difficulty levels found acceptance rates of [89.3% for easy problems, 72.1% for medium, and just 43.4% for hard problems](https://dl.acm.org/doi/10.1145/3715108). The domain of the problem matters too: Copilot achieved 90.1% acceptance on binary tree problems but only 49.5% on graph problems. This pattern is consistent across studies — LLMs perform well on tasks with clear patterns and well-represented training data, and performance degrades sharply as problems require novel algorithmic reasoning or domain-specific logic.

The quality of accepted code introduces a second layer of complexity. A [CodeRabbit analysis](https://www.coderabbit.ai/blog/state-of-ai-vs-human-code-generation-report) of 470 open-source GitHub pull requests found that AI-generated PRs averaged 10.83 issues each, compared with 6.45 issues in human-written PRs — approximately 1.7 times more. AI-generated code contained 1.4 times more critical issues and 1.7 times more major issues. A [large-scale study](https://arxiv.org/abs/2508.00700) comparing AI-generated and human-written code found that AI output tends to be simpler and more repetitive, yet more prone to unused constructs and hardcoded debugging artifacts, while human code exhibits greater structural complexity but different categories of maintainability problems. GitHub's own research paints a different picture, claiming that Copilot-assisted code showed [readability improvements of 3.62%, reliability gains of 2.94%, and maintainability improvements of 2.47%](https://github.blog/news-insights/research/does-github-copilot-improve-code-quality-heres-what-the-data-says/). The discrepancy likely reflects differences in methodology: vendor studies tend to measure within-task quality on controlled assignments, while independent analyses examine code quality in production repositories over time. This methodological divergence matters because it shapes the decisions engineering leaders make. If you rely on vendor data showing quality improvements, you might reduce code review investment. If you rely on independent data showing 1.7 times more issues, you might increase it. The stakes of getting this wrong are not theoretical — they show up in production incidents, security vulnerabilities, and mounting maintenance burden over the following months and years.

ZoomInfo's enterprise deployment study provides one of the most detailed real-world pictures available. The company conducted a [systematic four-phase evaluation](https://arxiv.org/html/2501.13282v1) involving over 400 developers. They found a 33% suggestion acceptance rate (20% measured by lines of code), with developers reporting approximately 20% time savings and 72% satisfaction scores. The study also surfaced the primary limitations that production engineering teams encounter: GitHub Copilot lacked domain-specific logic relevant to ZoomInfo's products and produced inconsistent code quality across different parts of the codebase. These are precisely the kinds of findings that controlled lab experiments cannot capture. The gap between a 33% acceptance rate in production and the 55% speedup in a controlled experiment reflects the difference between working on an isolated HTTP server task and navigating the accumulated complexity of a real product codebase with years of business logic, undocumented assumptions, and evolving requirements.

The question of whether accepted code is actually good code deserves scrutiny as well. GitHub reports that 88% of Copilot-generated code remains in the final version — a statistic frequently cited to support the tool's quality. But code staying in a repository is not the same as code being correct, secure, or maintainable. Code that is never refactored, never reviewed carefully, or simply not worth the effort of removing also stays in the codebase. The retention metric measures path of least resistance, not quality.

Developer experience level significantly shapes the productivity equation. Google's internal randomized controlled trial found that newer, less experienced developers reaped the most substantial benefits, with [speed improvements of 35% to 39%](https://arxiv.org/html/2410.12944v2), while seasoned developers saw smaller gains in the range of 8% to 16%. Microsoft's research suggests it takes approximately [11 weeks](https://resources.github.com/learn/pathways/copilot/essentials/measuring-the-impact-of-github-copilot/) for developers to fully realize productivity gains from AI coding tools, with teams often experiencing an initial productivity dip during the ramp-up period. This learning curve is frequently overlooked in ROI calculations that assume immediate productivity gains upon deployment. An organization that purchases 1,000 Copilot licenses and measures productivity two weeks later is measuring the ramp-up period, not the steady state. An organization that measures at week twelve may see gains that would not have been visible at week two. The timing of measurement shapes the conclusion as much as the measurement methodology itself — a subtlety that most vendor case studies elide entirely.

The experience-level finding also raises a question that is rarely asked in productivity discussions: productive for whom, at what cost? If junior developers achieve 35% speedups primarily by accepting code they do not fully understand, the short-term productivity gain may come at the expense of long-term learning. The 8% to 16% gains for experienced developers, while more modest, may represent a more sustainable form of productivity because those developers can evaluate what the tool produces and integrate it appropriately into their work. The best productivity metric is not the largest number but the one that accounts for the full cost of production, including the future costs of maintaining and evolving the code.

Test generation represents another area where the evidence is nuanced. LLMs can generate effective tests for simple code, but research consistently finds that [effectiveness decreases with code complexity](https://www.mdpi.com/2504-4990/7/3/97) and that AI-generated tests lack semantic diversity — they tend to test obvious paths rather than subtle edge cases. Coverage-guided approaches like [CoverUp](https://arxiv.org/html/2403.16218v3) have shown substantial improvements over baseline LLM generation, suggesting that the tool design matters as much as the underlying model capability.

The pattern that emerges across all of this code generation evidence is that LLMs function as powerful amplifiers of existing patterns. When the task involves reproducing well-established patterns — whether boilerplate Java, standard API calls, or basic test structures — the tools deliver measurable speed gains with acceptable quality. When the task requires reasoning about novel problems, navigating domain complexity, or making architectural trade-offs, the tools provide suggestions that look plausible but require significant human effort to validate and often correct. For engineering organizations, this means the productivity question is not whether to use code generation, but how to systematically identify which portions of your development work fall into each category and allocate AI assistance accordingly.

---

## The Debugging and Maintenance Story

Code generation captures the headlines, but debugging, maintenance, and code review consume the majority of engineering time. The evidence on LLM effectiveness in these areas reveals a more complex picture than the generation story, with genuine benefits sitting alongside equally genuine costs.

On the debugging front, academic research has made meaningful progress. Studies published in [ACM proceedings](https://dl.acm.org/doi/10.1145/3660773) and on arXiv demonstrate that LLMs can perform bug localization at the token level and subsequently apply repairs, with the separation of localization and fixing tasks enabling effective integration of diverse contextual information. Agent-based debugging systems show particular promise, though [comprehensive analyses](https://arxiv.org/html/2411.10213v2) find that both the underlying LLM and the design of the agentic workflow require further optimization. The technology works, but it works in a narrow band of well-defined bugs with clear reproduction paths. For the subtle architectural issues and concurrency bugs that consume the most senior engineering time, LLM debugging assistance remains limited. This is a crucial distinction for engineering leaders to understand: the bugs that are easiest for LLMs to find are also the bugs that are easiest for humans to find. The expensive bugs — the ones that take days of investigation, require understanding of distributed system behavior, or emerge only under specific production conditions — remain firmly in the domain of experienced human engineers.

Code review is where the tension between individual speed and organizational throughput becomes most visible. An [empirical study](https://arxiv.org/html/2505.16339v1) of LLM-assisted code review found that 73.8% of automated review comments were resolved by developers, suggesting that the tool identified real issues. However, the average pull request closure duration increased from five hours and 52 minutes to eight hours and 20 minutes. The automated comments added value, but they also added friction. Most practitioners in the study reported a minor improvement in overall code quality, with LLM-based tools proving useful for bug detection, quality awareness, and promotion of best practices. The net effect was positive but modest — a far cry from the transformation narrative. This finding deserves emphasis because code review is the primary quality gate for most engineering organizations. When the time saved writing code is partially or fully consumed by additional review time, the net productivity calculation shifts substantially. And the review cost scales with the volume of AI-generated code — a dynamic that means the review burden may worsen as AI tools become more capable and developers accept more suggestions.

Microsoft and Google have each adapted their code review processes to handle the growing volume of AI-generated code. Microsoft's internal experience with AI-powered code review gave the company [early exposure](https://devblogs.microsoft.com/engineering-at-microsoft/enhancing-code-quality-at-scale-with-ai-powered-code-reviews/) to the challenges of reviewing at scale, leading to the development of inline suggestions and human-in-the-loop review flows. These insights directly shaped the design of GitHub Copilot for Pull Request Reviews, which reached general availability in April 2025. Google followed a parallel path, integrating [Gemini Code Assist](https://www.coderabbit.ai/blog/ai-code-metrics-what-percentage-of-your-code-should-be-ai-generated) as a pull request reviewer that produces instant PR summaries, flags potential bugs and deviations from best practices, and suggests code changes. Both companies now handle roughly 30% AI-generated code in their review processes, and both have built explicit tooling to manage the resulting review burden.

That burden is substantial and growing. Research shows that reviews for Copilot-heavy pull requests take [26% longer](https://www.softwareseni.com/why-ai-coding-speed-gains-disappear-in-code-reviews/) as reviewers must check for AI-specific issues like inappropriate pattern usage and architectural misalignment. A [report analyzing AI-assisted pull requests](https://www.helpnetsecurity.com/2025/12/23/coderabbit-ai-assisted-pull-requests-report/) found that senior developers' original code contributions dropped by 19%, as their time was reallocated to managing a flood of lower-quality contributions. Their code review workload increased by 6.5%. At Salesforce, code volume [increased by approximately 30%](https://engineering.salesforce.com/scaling-code-reviews-adapting-to-a-surge-in-ai-generated-code/) after AI adoption, with pull requests regularly expanding beyond 20 files and 1,000 lines of change — well past the threshold where human reviewers can maintain attention and catch defects effectively.

The maintenance implications run deeper. GitClear's longitudinal analysis of [211 million changed lines of code](https://www.gitclear.com/ai_assistant_code_quality_2025_research) authored between 2020 and 2024 documents that code churn — the percentage of lines reverted or updated within two weeks of creation — has doubled compared to the pre-AI baseline. More concerning, the percentage of code classified as refactored ("moved") lines dropped from 24.1% in 2020 to just 9.5% in 2024, while copy-pasted lines [surged from 8.3% to 12.3%](https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality). AI tools appear to discourage refactoring by offering a one-keystroke temptation to generate new code rather than restructure existing code. MIT professor Armando Solar-Lezama summarized the concern: AI acts as a new credit card that allows engineering organizations to accumulate technical debt at unprecedented rates.

Google's code migration work provides a useful counterexample that illuminates where LLM-assisted maintenance genuinely shines. In a [well-documented internal project](https://arxiv.org/html/2501.06972v1), Google used AI to transition identifiers from 32-bit to 64-bit integers — a task that, done manually, had previously taken approximately two years. With AI writing nearly 75% of the migration code, developers estimated a [50% productivity increase](https://linearb.io/blog/how-google-uses-ai-to-speed-up-code-migrations). The key characteristic of this success is that the migration was highly structured, repetitive, and rule-based — exactly the kind of work where LLMs perform best. It stands in sharp contrast to the METR finding that experienced developers working on their own complex, context-heavy repositories were slowed down by AI tools.

The debugging and maintenance evidence, taken together, tells a story that should temper both enthusiasm and despair. LLMs are genuinely useful for code review triage, for catching common patterns of defects, and for automating large-scale mechanical changes. They are not yet useful — and may introduce net-negative effects — for the judgment-intensive work of maintaining complex, evolving systems. The organizations that extract the most value from these tools in maintenance contexts will be those that deploy them for specific, well-bounded tasks while preserving human judgment for the architectural and design decisions that shape long-term system health.

---

## Where LLMs Quietly Excel

While the debate over code generation productivity rages on, a less visible set of LLM applications may ultimately deliver higher returns for engineering organizations. Documentation, knowledge transfer, and information discovery receive far less attention than code completion, but the measured productivity gains in these areas are among the most consistent findings in the research.

The [Atlassian State of Developer Experience Report 2025](https://www.atlassian.com/teams/software-development/state-of-developer-experience-2025), surveying 3,500 developers and managers across six countries, identified the three most significant time-wasters in engineering work: finding information about services, documentation, and APIs; adapting to new technology; and context switching between tools. These are fundamentally knowledge problems, not coding problems, and they represent precisely the category where LLM tools deliver their most reliable value.

Atlassian's data is striking. In 2024, only 38% of developers reported any measurable time savings from AI tools. By 2025, that figure jumped to 99%, with [68% reporting savings of ten or more hours per week](https://www.atlassian.com/blog/developer/developer-experience-report-2025). The company attributes much of this gain to developers using AI for non-coding tasks — searching documentation, understanding unfamiliar codebases, generating explanations for code behavior, and producing documentation artifacts. Atlassian has invested in this capability through its own tooling: [Rovo Chat](https://www.atlassian.com/blog/atlassian-engineering/hybrid-llm) uses a hybrid approach combining multiple LLMs including models from OpenAI, Anthropic, Google, and Mistral. The Rovo Dev CLI ranked first on the SWE-bench benchmark for real-world issue resolution, and its pull request review functionality pulls directly from internal documentation to enforce coding standards and security policies.

Developer onboarding is an area where the evidence is particularly encouraging. A [case study conducted at LKS Next](https://conf.researchr.org/details/cseet-2024/cseet-2024-industry-track/4/Can-LLMs-Facilitate-Onboarding-Software-Developers-An-Ongoing-Industrial-Case-Study), an IT consulting firm, using Action Design Research methodology found that LLMs could effectively prioritize self-directed learning resources for new hires, reducing the burden on senior engineers who would otherwise serve as the primary knowledge source. The study noted important considerations around using third-party LLMs with proprietary code, but the fundamental finding — that AI tools can compress onboarding timelines by making codebases more self-explanatory — aligns with broader industry experience. When documentation is structured with consistent patterns and explicit annotations, LLMs are [significantly more likely to surface correct results](https://fusionauth.io/blog/llms-for-docs), creating a positive feedback loop between documentation quality and AI effectiveness.

Stripe's engineering organization illustrates a sophisticated approach to LLM deployment that goes well beyond code generation. The company has built [domain-specific foundation models](https://stripe.com/blog/engineering) for fraud detection, processing payments that represent approximately 1.3% of global GDP. Their card-testing fraud detection accuracy improved from 59% to 97% for their largest merchants — a concrete, measurable business outcome that makes the productivity conversation feel almost quaint by comparison. Their compliance investigation workflows use a decomposed architecture where complex reviews are broken into discrete tasks orchestrated by a directed acyclic graph, with LLMs functioning as constrained workers on defined rails. This pattern — using LLMs as structured knowledge processors within carefully defined boundaries — represents a fundamentally different philosophy from the open-ended code generation that dominates public discussion. It also suggests that the most valuable LLM applications in engineering may not be about writing code at all, but about processing, organizing, and applying the organizational knowledge that enables good engineering decisions.

The paradox that Atlassian's report reveals is instructive. Developers are saving ten hours per week using AI, predominantly on knowledge and documentation tasks. They are simultaneously losing ten or more hours per week to organizational friction that AI tools do not address: fragmented information architectures, poorly maintained internal documentation, and the cognitive overhead of context switching across toolchains. Sixty-three percent of developers now say that [engineering leaders do not understand their pain points](https://www.atlassian.com/blog/developer/developer-experience-report-2025), up sharply from 44% the previous year. This suggests that organizations are banking AI-driven time savings without investing those savings into the systemic improvements that would compound them — a pattern that echoes the DORA finding that individual speed gains do not automatically translate into organizational delivery improvements.

The implication for engineering leaders is that the highest-ROI LLM investments may not be in code generation at all. When you reduce the time engineers spend searching for information, understanding unfamiliar systems, and producing documentation, you remove friction that affects every task rather than accelerating a single task type. A [GitHub survey found that over 97% of 2,000 developers](https://fusionauth.io/blog/llms-for-docs) had used AI tools at work in some capacity, with documentation generation and code explanation among the most consistently valued applications. The organizations seeing the greatest returns appear to be those that treat LLMs as knowledge infrastructure rather than code factories.

There is also a compounding effect that makes documentation and knowledge investments particularly attractive. When AI tools help an engineering organization maintain better documentation, that improved documentation in turn makes the AI tools more effective at answering future questions, onboarding future hires, and supporting future code reviews. This virtuous cycle does not exist in code generation, where the quality of AI output depends primarily on the model and the task rather than on organizational investment. Engineering teams that recognize this dynamic and invest accordingly will build an advantage that accumulates over time, while those focused exclusively on code generation speed will see their gains plateau as the underlying bottlenecks shift to knowledge and coordination problems.

---

## The Costs That Don't Show Up in Benchmarks

Every vendor benchmark measures what gets faster. Almost none measure what gets worse. The growing body of independent research on the second-order effects of LLM adoption paints a picture that engineering leaders ignore at their peril.

GitClear's analysis of [211 million changed lines of code](https://www.gitclear.com/ai_assistant_code_quality_2025_research) across repositories owned by Google, Microsoft, Meta, and enterprise corporations provides the most comprehensive longitudinal view of code quality trends since widespread AI adoption. The headline finding is that AI-generated code has a [41% higher churn rate](https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality) than human-written code — meaning it is more likely to be reverted or substantially modified within two weeks of creation. Code churn overall has doubled compared to the 2021 pre-AI baseline. But the composition metrics tell a more concerning story. The percentage of changed lines associated with refactoring — code being thoughtfully reorganized and improved — dropped from 25% in 2021 to less than 10% in 2024. Meanwhile, lines classified as copy-pasted surged from 8.3% in 2020 to 12.3% in 2024, a 48% relative increase. The [2025 update](https://www.gitclear.com/ai_assistant_code_quality_2025_research) documented a fourfold growth in code clones. AI tools make it trivially easy to generate new code rather than reuse existing abstractions, and developers appear to be accepting that trade-off at scale.

The security implications are equally sobering. Stanford researchers led by Professor Dan Boneh [found](https://ee.stanford.edu/dan-boneh-and-team-find-relying-ai-more-likely-make-your-code-buggier) that developers who used AI coding assistants wrote significantly less secure code than those working without assistance. Perhaps more concerning, participants with AI access were more likely to believe their code was secure — a false confidence effect that compounds the problem. The researchers noted that developers who trusted AI less and engaged more deliberately with their prompts produced fewer security vulnerabilities, suggesting that the risk is not inherent to the tools but to the way developers interact with them.

Veracode's [2025 GenAI Code Security Report](https://www.veracode.com/resources/analyst-reports/2025-genai-code-security-report/) provides the most comprehensive security assessment to date. The company tested over 100 LLMs across 80 code completion tasks with known potential for security vulnerabilities. The result: AI-generated code introduced risky security flaws in [45% of tests](https://www.veracode.com/blog/ai-generated-code-security-risks/). The failure rates by vulnerability category are stark. Models failed to generate secure code 86% of the time for Cross-Site Scripting, 88% for Log Injection, and 20% for SQL Injection. Java was the riskiest language with a 72% security failure rate across tasks. The finding that larger models performed no better than smaller models on security metrics suggests this is a systemic problem with current training approaches, not one that will be solved simply by scaling up. For engineering organizations, this has immediate practical implications: even if you upgrade from one generation of AI coding assistant to the next, you should not assume that security properties improve. Security review processes for AI-generated code need to be at least as rigorous as those for human-written code, and arguably more so given the false confidence effect documented in the Stanford research.

The impact on junior developer skill acquisition is an area where research is still emerging, but early signals are concerning. The [systematic literature review of 37 studies](https://arxiv.org/abs/2507.03156) identified cognitive offloading as a recurring concern: when developers delegate routine problem-solving to AI tools, they may lose the deliberate practice that builds foundational skills. Google's RCT data shows the tension clearly — newer developers benefited most from AI assistance, with [35% to 39% speed improvements](https://arxiv.org/html/2410.12944v2), but they are also the developers who stand to lose the most if foundational skill development is short-circuited. The METR study reinforced this dynamic from the other direction, noting that the one developer with more than 50 hours of Cursor experience actually saw a speedup, suggesting there is a high skill ceiling for effective AI tool use. If you have not developed strong engineering judgment first, AI tools may accelerate you toward the wrong solutions. The parallel to GPS navigation is apt: experienced drivers who already know the road network use GPS to optimize routes, while novice drivers who rely entirely on GPS sometimes develop a fragile understanding of geography that collapses when the device is unavailable. In software engineering, the stakes are higher because the "wrong route" is a security vulnerability, architectural dead end, or maintenance nightmare that may not become visible for months.

The deskilling concern is not merely theoretical. The [Atlassian 2025 DevEx report](https://www.atlassian.com/teams/software-development/state-of-developer-experience-2025) found that developers are increasingly spending their time on AI-mediated tasks rather than direct engagement with code. When younger developers report that they save ten or more hours a week using AI, the question is what they would have learned during those ten hours — and whether that learning represented disposable friction or essential skill building. The answer is almost certainly both, and the challenge for engineering organizations is distinguishing between the two without either blocking genuine productivity gains or enabling long-term capability erosion.

The code review burden represents the cost most directly visible to engineering teams today. [AI-generated pull requests contain 1.7 times more issues](https://www.coderabbit.ai/blog/state-of-ai-vs-human-code-generation-report) than human-written PRs, with 1.4 times more critical issues. Reviews for Copilot-heavy PRs take [26% longer](https://www.softwareseni.com/why-ai-coding-speed-gains-disappear-in-code-reviews/), and senior developers' original code contributions have [dropped by 19%](https://www.helpnetsecurity.com/2025/12/23/coderabbit-ai-assisted-pull-requests-report/) as their time shifts toward managing the resulting review volume. According to the [Harness State of Software Delivery 2025](https://devops.com/ai-in-software-development-productivity-at-the-cost-of-code-quality-2/) report, 67% of developers now spend more time debugging AI-generated code than they would spend writing the code manually. The irony is not subtle: tools marketed as productivity multipliers are, in many contexts, shifting work from writing (which developers find satisfying) to reviewing and debugging (which they find draining).

The over-reliance trap is perhaps the most insidious cost because it operates below the level of conscious awareness. The METR study's [39-percentage-point perception gap](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) — developers believed they were faster, but they were actually slower — suggests that AI tools may compromise the metacognitive abilities developers rely on to evaluate their own effectiveness. The researchers identified specific mechanisms: over-optimism about AI capabilities, AI-generated suggestions interfering with developers' existing mental models of the codebase, poor LLM performance on repositories with significant tacit knowledge, and low reliability of generated code requiring extensive verification. When your tools make you feel productive while making you slower, the path to correction is unusually difficult because the feedback signal is inverted.

What makes this collection of costs particularly challenging for engineering leaders is that most of them are invisible in the metrics that organizations typically track. Code review time, technical debt accumulation, security vulnerability introduction, and skill atrophy are all lagging indicators. By the time they surface in dashboards, the organizational habits that produced them are already entrenched. The vendor benchmark that showed a 55% speedup is a leading indicator of adoption decisions; the GitClear analysis showing doubled code churn is a trailing indicator of the consequences. Effective LLM strategy requires tracking both simultaneously — and making the difficult decision to slow down adoption when trailing indicators signal problems, even when leading indicators still look compelling.

---

## What the Most Effective Teams Do Differently

The evidence reviewed so far might seem to support either uncritical enthusiasm or wholesale skepticism about LLM tools in engineering. But the organizational case studies tell a more nuanced story: the teams reporting the highest productivity gains from LLM adoption share specific practices that distinguish them from teams where adoption produces neutral or negative results.

Palo Alto Networks provides one of the most thoroughly documented enterprise deployments. The company onboarded [2,000 developers](https://aws.amazon.com/partners/success/palo-alto-networks-anthropic-sourcegraph/) within three months and achieved productivity increases of up to 40%, with an average of 25%, using Sourcegraph's Cody assistant powered by Anthropic's Claude 3.5 Sonnet on Amazon Bedrock. But the gains did not come from simply giving developers access to a tool. Palo Alto Networks followed a deliberately phased rollout: they tested a prototype for one month, then selected 150 core product developers working in different roles, countries, and programming languages to continue the pilot for two additional months. Only after iterative testing and feedback did they open the tool to more than 1,000 developers. Crucially, the deployment included targeted training from three separate vendors — AWS provided AI fundamentals training, Anthropic delivered prompt engineering workshops, and Sourcegraph ran Cody-specific learning sessions. The organizational investment in training was treated as integral to the deployment, not as an afterthought.

This phased approach aligns with what a [study of 22 software practitioners](https://arxiv.org/html/2511.06428v1) found about successful LLM adoption patterns. The research identified that implementation approach matters more than technology selection — organizations using structured frameworks achieved faster time-to-value through production-ready infrastructure. Benefits that practitioners reported included maintaining software development flow, improving developers' mental models of their systems, and fostering a more entrepreneurial approach to problem-solving. The disadvantages clustered around specific, predictable failure modes that proper process design can mitigate. This is perhaps the most actionable finding in the organizational research: the failure modes of LLM adoption are not random or mysterious. They are predictable, they have been documented across multiple studies, and organizations that design their deployment processes to address them can achieve significantly better outcomes than those that leave developers to figure it out independently.

The contrast between Palo Alto Networks' experience and the METR study's results is instructive. METR's developers were experienced but working individually, without organizational support structures or training specific to the AI tools. Palo Alto Networks invested in targeted training, started with low-stakes projects, and expanded gradually with measurement at each stage. The difference in outcomes — 25% average improvement versus 19% slowdown — cannot be attributed to the tools alone. It reflects the organizational infrastructure surrounding the tools.

Adapted code review processes represent perhaps the single most important organizational adjustment. Microsoft's internal experience with AI-powered code reviews led to the development of [human-in-the-loop review flows](https://devblogs.microsoft.com/engineering-at-microsoft/enhancing-code-quality-at-scale-with-ai-powered-code-reviews/) where AI handles first-pass issue detection and reviewers focus on architectural alignment and business logic correctness. This is not just a process improvement — it is a fundamental redefinition of the reviewer's role from line-by-line inspection to higher-order judgment. Google followed a similar path with Gemini Code Assist, integrating AI as a [first-line reviewer](https://www.coderabbit.ai/blog/ai-code-metrics-what-percentage-of-your-code-should-be-ai-generated) that generates PR summaries, flags potential bugs, and suggests changes before human reviewers engage. Both companies now treat code review policies for AI-generated code as an explicit, documented practice rather than leaving individual reviewers to develop their own heuristics.

The role of prompt engineering as an organizational capability cannot be overstated. While [72% of companies](https://dextralabs.com/blog/prompt-engineering-for-llm/) had integrated AI into at least one business function by 2024, 74% were still struggling to achieve and scale AI value. The prompt engineering market is growing from $280 million in 2024 to a projected $2.5 billion by 2032, reflecting growing recognition that effective AI use requires learned skill, not just tool access. Research consistently finds that prompt engineering techniques — zero-shot, few-shot, chain-of-thought prompting — can handle approximately [85% of the work](https://www.promptingguide.ai/) in many development contexts, but only when developers know how to apply them. Palo Alto Networks' decision to invest in dedicated prompt engineering training from Anthropic appears to have directly contributed to their above-average productivity gains.

Mentorship adaptation is the organizational change with the longest time horizon and the least immediate visibility. Successful teams are developing hybrid approaches where senior developers [define explicit boundaries](https://www.qodo.ai/blog/how-to-mentor-a-junior-developer-ultimate-guide/) for when AI use is acceptable and when manual work is mandatory, while letting junior developers paired with AI tools handle routine tasks. The focus of mentorship shifts from teaching syntax and patterns — which AI handles effectively — toward system thinking, cross-functional communication, and the kind of complex architectural reasoning that LLMs consistently struggle with. This is not simply adding AI to existing mentorship structures; it is a fundamental rethinking of what junior developers need to learn and how senior developers allocate their teaching time. The stakes are high: if an entire generation of developers learns to write software by accepting AI suggestions rather than by understanding problems deeply, the industry may face a talent pipeline crisis in five to ten years when those developers are expected to architect systems, debug complex production incidents, and mentor the next cohort. The organizations investing in hybrid mentorship models now are placing a bet on long-term engineering capability that will not pay off in quarterly productivity metrics but may prove decisive in talent quality over time.

Booking.com's measurement-first approach deserves emphasis as an organizational pattern. By [partnering with DX](https://getdx.com/customers/booking-uses-dx-to-measure-impact-of-genai/) to track impact across throughput, satisfaction, and task reallocation simultaneously, the company avoided the single-metric trap that has misled so many vendor studies. Their finding that daily AI users had 16% higher throughput comes with the context that satisfaction also improved, and that time savings were redirected to higher-value work rather than simply producing more code of the same type. This kind of multi-dimensional measurement, grounded in established frameworks like SPACE and DORA, represents the difference between organizations that can optimize their AI investment over time and those operating on vendor-supplied faith.

The common thread across all of these successful deployments is intentionality. None of the organizations that reported significant, sustained productivity gains simply purchased licenses and waited for results. Every one of them invested in training, adapted their processes, measured across multiple dimensions, and iterated their approach based on evidence. The tools themselves are necessary but not sufficient — and any vendor pitch that implies otherwise is, at best, oversimplifying a complex organizational change management challenge. The [MIT Technology Review](https://www.technologyreview.com/2025/12/15/1128352/rise-of-ai-coding-developers-2026/) reported that the rise of AI coding assistants is now ubiquitous, but organizational confidence in the results varies enormously. The variance is explained not by which tool an organization selected, but by how deliberately they deployed it.

---

## Building an Evidence-Based LLM Strategy

After reviewing more than 40 studies, the evidence converges on a set of conclusions that are less dramatic than vendor marketing but more useful than reflexive skepticism. LLM coding assistants deliver genuine productivity gains in specific, well-bounded task categories. They introduce measurable costs in others. And the organizational practices surrounding deployment matter at least as much as the tools themselves.

The research most consistently supports LLM adoption for documentation generation and knowledge work, where [Atlassian's data](https://www.atlassian.com/teams/software-development/state-of-developer-experience-2025) shows near-universal time savings and the risk profile is low. Boilerplate code generation shows strong evidence across multiple studies, with high acceptance rates and manageable quality impact for well-defined, repetitive patterns. Code migration and large-scale refactoring show impressive results in structured contexts — Google's [75% AI-authored migration code](https://arxiv.org/html/2501.06972v1) with 50% productivity gains represents the ceiling for the right kind of task. Test scaffolding and onboarding acceleration round out the high-confidence category, with consistent evidence of time savings when used as starting points rather than finished products.

The mixed-evidence category includes overall net productivity, where Google's [21% improvement](https://arxiv.org/html/2410.12944v2) and METR's [19% slowdown](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) bracket a wide range of possible outcomes depending on task complexity, developer experience, and organizational context. Code review assistance falls here as well — useful for first-pass detection but sometimes [increasing overall review time](https://arxiv.org/html/2505.16339v1). Debugging assistance shows genuine promise in controlled settings but limited effectiveness on the complex, context-heavy bugs that consume the most engineering time.

Organizations should proceed with explicit caution in several areas. Security-sensitive code is the clearest: Veracode's finding that [45% of AI-generated code contains vulnerabilities](https://www.veracode.com/resources/analyst-reports/2025-genai-code-security-report/), combined with Stanford's research showing that AI users produce less secure code while [believing it to be more secure](https://ee.stanford.edu/dan-boneh-and-team-find-relying-ai-more-likely-make-your-code-buggier), creates a risk profile that demands deliberate mitigation. Junior developer reliance without structured mentorship invites the deskilling effects identified across multiple studies. Architecture decisions on large, context-heavy codebases remain areas where AI tools [actively interfere with developer effectiveness](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/). And any task requiring deep domain knowledge — the kind of institutional understanding that comes from years of working within a specific business context — will see limited benefit from tools trained on public code that cannot access the tacit knowledge embedded in organizational decisions.

For engineering leaders evaluating LLM tool adoption, the evidence points toward a framework built on several principles. First, measure across the full development lifecycle using established frameworks like [SPACE](https://queue.acm.org/detail.cfm?id=3454124) and [DORA](https://dora.dev/guides/dora-metrics-four-keys/), not just code generation speed. Track downstream costs — code churn, review time, defect rates — alongside upstream gains. Second, deploy in phases with measurement at each stage, following the Palo Alto Networks pattern of starting with low-risk use cases, expanding gradually, and investing in training alongside tooling. Third, establish explicit policies for AI-generated code in code review, adapting the human-in-the-loop patterns that Microsoft and Google have developed internally. Fourth, protect junior developer learning by establishing boundaries between tasks where AI assistance is appropriate and tasks where deliberate practice matters more than speed.

Several open research questions will shape the next two years of evidence. The longest-running longitudinal studies are only now beginning to capture the cumulative effects of AI-generated code on codebase maintainability. The optimal task boundaries for AI assistance remain poorly defined beyond broad categories. Second-order effects at organizational scale — how AI adoption in one team affects adjacent teams' workflows — have barely been studied. And the question of whether and how to prevent skill atrophy among engineers who spend their formative years working alongside AI assistants remains almost entirely open. Perhaps most urgently, the field lacks agreed-upon methodologies for comparing results across studies — the reason a Google study can show 21% improvement while a METR study shows 19% degradation is partly a function of genuinely different populations and tasks, but also partly a function of incompatible measurement approaches.

The [DX research team](https://getdx.com/research/measuring-ai-code-assistants-and-agents/) has proposed more comprehensive frameworks for measuring AI code assistants and agents that account for the full spectrum of developer activity. These emerging frameworks attempt to reconcile the speed-focused metrics that vendors favor with the quality and sustainability metrics that independent researchers highlight. If widely adopted, they could provide the common measurement language that the field currently lacks — and that engineering leaders desperately need in order to make informed investment decisions.

What the data does tell us is that the trajectory is toward deeper integration, not retreat. Google is heading toward [50% or more AI-generated code](https://medium.com/@thedailyautomationmindset/ai-now-writes-50-of-googles-code-what-s-really-happening-8706d70fb328). Replit's agent-first platform created [5 million applications in 2025](https://blog.replit.com/2025-replit-in-review). The industry is moving from code completion toward full agentic workflows that manage entire development tasks autonomously. Organizations that develop evidence-based evaluation practices now — measuring real impact, adapting processes, investing in human capabilities alongside tool capabilities — will be positioned to benefit from this trajectory. Those that adopt on faith, measuring only what makes the investment look good, will discover the hidden costs when the technical debt comes due.

The most important finding across this entire body of research may be the simplest: the tools work, but not the way the marketing says they work. They accelerate specific tasks, not entire workflows. They help most when bounded, structured, and paired with explicit process adaptation. And they require ongoing, honest measurement to distinguish genuine productivity from the comfortable illusion of it.

The 55% speedup and the 19% slowdown are not contradictory findings. They are measurements of different things, taken under different conditions, reflecting different aspects of what it means to be a productive software engineer. The field is just beginning to develop the frameworks needed to reconcile them. Engineering leaders who understand this nuance — and act on it — will make better investment decisions, build more effective teams, and produce higher-quality software than those who adopt based on headlines alone. The evidence is clear enough to act on, even as it remains too mixed to reduce to a simple verdict. That discomfort is not a reason to delay adoption. It is a reason to adopt thoughtfully, measure honestly, and adjust continuously as the evidence base matures.
